{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    try:\n",
    "        return loadmat(filename)\n",
    "    except TypeError:\n",
    "        print(\"Not a valid filename argument: \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derive=False):\n",
    "    if derive:\n",
    "        return x * (1 - x)\n",
    "\n",
    "    return 1/(1 + np.exp((-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    sig_z = sigmoid(z)\n",
    "    return np.multiply(sig_z, (1 - sig_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, theta_list):\n",
    "\n",
    "    m = X.shape[0]\n",
    "    a_list = []\n",
    "    z_list = []\n",
    "    \n",
    "    #for thera in theta_list:\n",
    "    #     print(thera.shape)\n",
    "    \n",
    "    for idx, thera in enumerate(theta_list):\n",
    "        if idx == 0:\n",
    "            #print(\"***** forward if ********\")\n",
    "            a_list.append(np.insert(X, 0, values=np.ones(m), axis=1))\n",
    "            #print(a_list[0].shape)\n",
    "            #print(theta_list[0].T.shape)\n",
    "            #print((a_list[0] * (theta_list[0].T)).shape)\n",
    "            z_list.append(a_list[0] * (theta_list[0].T))\n",
    "            #print(\"***** forward if end********\")\n",
    "        else:\n",
    "            a_list.append(np.insert(sigmoid(z_list[idx-1]), 0, values=np.ones(m), axis=1))\n",
    "            #print(\"***** forward else ********\")\n",
    "            #print(z_list[idx-1].shape)\n",
    "            #print(a_list[idx].shape)\n",
    "            #print(theta_list[idx].T.shape)\n",
    "            #print(\"***** forward else end********\")\n",
    "            temp = a_list[idx] * theta_list[idx].T\n",
    "            z_list.append(a_list[idx] * theta_list[idx].T)\n",
    "\n",
    "    h = sigmoid(z_list[len(z_list)-1])\n",
    "\n",
    "    return a_list, z_list, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(params, input_size, hidden_layers, num_labels, X, y, learning_rate, regularize = False):\n",
    "\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    theta_list = []\n",
    "    startCount = 0\n",
    "    for idx, val in enumerate(hidden_layers):\n",
    "        if idx == 0:\n",
    "            startCount = val * (input_size + 1)\n",
    "            theta_list.append(np.matrix(np.reshape(params[:startCount], (val, (input_size + 1)))))\n",
    "        if idx != 0:\n",
    "            tempCount = startCount\n",
    "            startCount += (val * (hidden_layers[idx-1] + 1))\n",
    "            theta_list.append(np.matrix(np.reshape(params[tempCount:startCount], (val, (hidden_layers[idx-1] + 1)))))\n",
    "        if idx == (len(hidden_layers)-1):\n",
    "            theta_list.append(np.matrix(np.reshape(params[startCount:], (num_labels, (val + 1)))))\n",
    "\n",
    "\n",
    "    a_list, z_list, h = forward_prop(X, theta_list)\n",
    "\n",
    "    delta_list = []\n",
    "    for theta in theta_list:\n",
    "        delta_list.append(np.zeros(theta.shape))\n",
    "        \n",
    "    J = cost(X, y, h, theta_list, learning_rate, regularize)\n",
    "    \n",
    "    #print(\"***** a,z values ********\")\n",
    "    #for aa in a_list:\n",
    "        #print(aa.shape)\n",
    "        \n",
    "    #for zz in z_list:\n",
    "        #print(zz.shape)\n",
    "    #print(\"***** a,z values end ********\")\n",
    "    \n",
    "    d_list = []\n",
    "    d_list.append(h - y)\n",
    "    \n",
    "    for idx, theta in reversed(list(enumerate(theta_list))):\n",
    "        if idx != 0:\n",
    "            #print(idx)\n",
    "            z_list[len(z_list)-1-idx] = np.insert(z_list[len(z_list)-1-idx], 0, values=np.ones(1), axis=1)\n",
    "            #print(((theta.T) * (d_list[len(d_list)-1].T)).T.shape)\n",
    "            #print(sigmoid_gradient(z_list[len(z_list)-1-idx]).shape)\n",
    "            #print(z_list[len(z_list)-1-idx].shape)\n",
    "            d_list.append(np.multiply(( (theta.T) * (d_list[len(d_list)-1].T) ).T, sigmoid_gradient(z_list[len(z_list)-1-idx])))\n",
    "\n",
    "    #print(len(d_list))\n",
    "    d_list.reverse()\n",
    "\n",
    "    #print(len(d_list))\n",
    "    for idx, delta in enumerate(delta_list):\n",
    "        #print(idx)\n",
    "        if idx == (len(delta_list) - 1):\n",
    "            #print((d_list[idx].T).shape)\n",
    "            #print(a_list[idx].shape)\n",
    "            delta_list[idx] += (d_list[idx].T) * a_list[idx]\n",
    "        else:\n",
    "            #print((d_list[idx][:, 1:].T).shape)\n",
    "            #print(a_list[idx].shape)\n",
    "            delta_list[idx] += (d_list[idx][:, 1:].T) * a_list[idx]\n",
    "            \n",
    "        delta[idx] = delta[idx] / m\n",
    "\n",
    "\n",
    "   \n",
    "    if regularize:\n",
    "        for idx, delta in enumerate(delta_list):\n",
    "            delta_list[idx][:, 1:] = delta_list[idx][:, 1:] + (theta_list[idx][:, 1:] * learning_rate) / m\n",
    "\n",
    "    \n",
    "    grad = np.concatenate((np.ravel(delta_list[0]), np.ravel(delta_list[1])))\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, y, h, theta_list, learning_rate, regularize=False):\n",
    "\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "\n",
    "    J = (np.multiply(-y, np.log(h)) - np.multiply((1 - y), np.log(1 - h))).sum() / m\n",
    "\n",
    "        \n",
    "    if regularize:\n",
    "        regularization_value = 0.0\n",
    "        for theta in theta_list:\n",
    "            regularization_value += np.sum(np.power(theta[:, 1:], 2))\n",
    "        J += (float(learning_rate) / (2 * m)) * regularization_value\n",
    "        \n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_net2():\n",
    "\n",
    "    input_size = 400\n",
    "    hidden_layers = [25]\n",
    "    num_labels = 10\n",
    "    learning_rate = 1\n",
    "\n",
    "    data = load_data('data/ex3data1.mat')\n",
    "    X = data['X']  \n",
    "    y = data['y']  \n",
    "\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    y_encoded = encoder.fit_transform(y)\n",
    "    \n",
    "    print(y_encoded.shape)\n",
    "    \n",
    "    total_param_count = 0\n",
    "    for idx, val in enumerate(hidden_layers):\n",
    "        if idx == 0:\n",
    "            total_param_count += val * (input_size + 1)\n",
    "        if idx != 0:\n",
    "            total_param_count += val * (hidden_layers[idx-1] + 1)\n",
    "        if idx == (len(hidden_layers)-1):\n",
    "            total_param_count += num_labels * (val + 1)\n",
    "    \n",
    "\n",
    "    params = (np.random.random(size=total_param_count) - 0.5) * 0.25\n",
    "    \n",
    "    print(\"Running the backpropagation algorithm...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    fmin = minimize(fun=back_prop, x0=params, args=(input_size, hidden_layers, num_labels, X, y_encoded, learning_rate),\n",
    "                    method='TNC', jac=True, options={'maxiter': 250})\n",
    "   \n",
    "\n",
    "    print(\"Result: \", fmin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400) (5000, 1)\n",
      "(5000, 10)\n",
      "Running the backpropagation algorithm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-d192428f060f>:7: RuntimeWarning: divide by zero encountered in log\n",
      "  J = (np.multiply(-y, np.log(h)) - np.multiply((1 - y), np.log(1 - h))).sum() / m\n",
      "<ipython-input-7-d192428f060f>:7: RuntimeWarning: invalid value encountered in multiply\n",
      "  J = (np.multiply(-y, np.log(h)) - np.multiply((1 - y), np.log(1 - h))).sum() / m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:       fun: 2.567584339495157\n",
      "     jac: array([8.85592837e-03, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       3.07608117e+02, 3.07629241e+02, 3.07517753e+02])\n",
      " message: 'Linear search failed'\n",
      "    nfev: 144\n",
      "     nit: 6\n",
      "  status: 4\n",
      " success: False\n",
      "       x: array([ 0.01280054,  0.05203696, -0.05227382, ..., -0.14104095,\n",
      "       -0.07289643, -0.04257702])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "run_net2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
